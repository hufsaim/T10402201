{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3jPZ5uhBuKu"
      },
      "source": [
        "## Linear Regression Model\n",
        "\n",
        "- 이번 실습는 linear regression model을 직접 구현하여 mini-batch stochastic gradient descent 방법을 이용해서 model을 학습시켜보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OdKcgl3138T3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numpy & PyTorch\n",
        "- y = Xw + b 연산을 numpy와 pytorch에서 각각 계산해보고 유사성을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: [[1. 0. 0.]\n",
            " [0. 2. 0.]]\n",
            "w: [2. 4. 8.]\n",
            "b: [1.]\n",
            "y: [3. 9.]\n"
          ]
        }
      ],
      "source": [
        "# numpy \n",
        "\n",
        "x1 = np.array([1.,0,0])\n",
        "x2 = np.array([0,2.0,0])\n",
        "\n",
        "X = np.zeros((2,3))\n",
        "X[0,:] = x1\n",
        "X[1,:] = x2\n",
        "\n",
        "w = np.array([2.,4,8])\n",
        "b = np.array([1.])\n",
        "\n",
        "y = np.matmul(X,w)+b \n",
        "\n",
        "print('X:',X)\n",
        "print('w:',w)\n",
        "print('b:',b)\n",
        "print('y:',y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: tensor([[1., 0., 0.],\n",
            "        [0., 2., 0.]])\n",
            "w: tensor([2., 4., 8.])\n",
            "b: tensor(1.)\n",
            "y: tensor([3., 9.])\n"
          ]
        }
      ],
      "source": [
        "# pytorch\n",
        "\n",
        "x1 = torch.tensor([1.,0,0])\n",
        "x2 = torch.tensor([0,2.,0])\n",
        "\n",
        "X = torch.zeros((2,3))\n",
        "X[0,:] = x1\n",
        "X[1,:] = x2\n",
        "\n",
        "w = torch.tensor([2.,4.,8.])\n",
        "b = torch.tensor(1.)\n",
        "\n",
        "y = torch.matmul(X,w) + b\n",
        "\n",
        "print('X:',X)\n",
        "print('w:',w)\n",
        "print('b:',b)\n",
        "print('y:',y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- numpy array와 pytorch tensor 사이의 연산을 확인합니다.\n",
        "- a+b는 안되는데 b+a는 되는 이유는 무엇일까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/n7/jq4g2jg94p31535vqfnh88b40000gn/T/ipykernel_48315/2493820479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(a+b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(a-b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'"
          ]
        }
      ],
      "source": [
        "a = np.array([1,2,3.])\n",
        "b = torch.tensor([1,2,3.])\n",
        "\n",
        "#print(a+b)\n",
        "#print(a-b)\n",
        "print(a*b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2., 4., 6.], dtype=torch.float64)\n",
            "tensor([0., 0., 0.], dtype=torch.float64)\n",
            "tensor([1., 4., 9.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "a = np.array([1,2,3.])\n",
        "b = torch.tensor([1,2,3.])\n",
        "\n",
        "print(b+a)\n",
        "print(b-a)\n",
        "print(b*a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Linear Regression Model\n",
        "- Matrix X는 각 row에 각각의 example이 가지고 있는 input feature들을 포함합니다.\n",
        "- vector w는 각각의 feature에 해당하는 weight입니다.\n",
        "- b는 bias로 모든 feature가 0일 때의 출력값입니다.\n",
        "- w와 b는 우리가 학습을 시켜야 되는 파라미터들이기 때문에 requires_grad = True 를 포함시켜야 합니다. \n",
        "- pytorch에서는 requires_grad = True인 tensor들에 대해 자동으로 gradient를 계산할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# linear regression model y = Xw + b\n",
        "def linreg(X, w, b):\n",
        "    return torch.matmul(X,w) + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BipFylj83-5y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "w: tensor([[ 0.0123],\n",
            "        [ 0.0120],\n",
            "        [-0.0018]], requires_grad=True)\n",
            "b: tensor([0.], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "X = torch.eye(3)\n",
        "w = torch.normal(0,0.01,size=(3,1),requires_grad = True)\n",
        "#w = torch.tensor([1.,1,1],requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "\n",
        "print(\"X:\",X)\n",
        "print(\"w:\",w)\n",
        "print(\"b:\",b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y: tensor([[ 0.0123],\n",
            "        [ 0.0120],\n",
            "        [-0.0018]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "y = linreg(X,w,b)\n",
        "print(\"y:\",y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Squared Loss\n",
        "- tensor의 연산에서 tensor의 shape를 맞추는 것은 매우 중요합니다.\n",
        "- tensor의 shape가 맞지 않는 경우 임의로 broadcasting이 진행되어, 의도하지 않은 결과를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.1380, 4.2715, 9.4051],\n",
            "        [1.0535, 4.1063, 9.1591],\n",
            "        [0.6351, 3.2289, 7.8228]])\n"
          ]
        }
      ],
      "source": [
        "y_hat = torch.normal(0,0.1,size=(3,1))\n",
        "y = torch.tensor([1.,2,3])\n",
        "\n",
        "print((y-y_hat)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5872],\n",
            "        [2.1540],\n",
            "        [4.6670]])\n"
          ]
        }
      ],
      "source": [
        "y_hat = torch.normal(0,0.1,size=(3,1))\n",
        "y = torch.tensor([1.,2,3],)\n",
        "\n",
        "\n",
        "print( (y_hat-y.reshape(y_hat.shape))**2 /2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def squared_loss(y_hat, y):\n",
        "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5872],\n",
              "        [2.1540],\n",
              "        [4.6670]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squared_loss(y_hat,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 정의한 loss function을 이용하여 w, b에 대한 gradient를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# initial parameters\n",
            "tensor([1., 1., 1.], requires_grad=True) tensor([0.], requires_grad=True)\n",
            "# initial gradients: None\n",
            "None None\n",
            "# first prediction using initial parameters\n",
            "tensor([1., 1., 1.], grad_fn=<AddBackward0>)\n",
            "# calculated loss\n",
            "tensor(2.5000, grad_fn=<SumBackward0>)\n",
            "# calculated gradients using the loss\n",
            "tensor([ 0., -1., -2.]) tensor([-3.])\n"
          ]
        }
      ],
      "source": [
        "y = torch.tensor([1.,2,3]) # label (ground truth)\n",
        "X = torch.eye(3)\n",
        "w = torch.tensor([1.,1,1],requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "\n",
        "print('# initial parameters')\n",
        "print(w,b)\n",
        "print('# initial gradients: None')\n",
        "print(w.grad, b.grad)\n",
        "\n",
        "y_hat = linreg(X,w,b)\n",
        "print('# first prediction using initial parameters')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "print('# calculated loss')\n",
        "print(l.sum())\n",
        "\n",
        "print('# calculated gradients using the loss')\n",
        "print(w.grad, b.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 계산된 gradients를 이용해 파라미터를 새로 업데이트 합니다.\n",
        "- 파라미터를 업데이트한 후에는 각각의 파라미터의 gradient를 다시 초기화 해주어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([1., 1., 1.], requires_grad=True), tensor([0.], requires_grad=True)]\n",
            "[tensor([ 0., -1., -2.]), tensor([-3.])]\n",
            "[tensor([1.0000, 1.3333, 1.6667], requires_grad=True), tensor([1.], requires_grad=True)]\n",
            "[tensor([0., 0., 0.]), tensor([0.])]\n"
          ]
        }
      ],
      "source": [
        "lr = 1\n",
        "batch_size = 3\n",
        "print([w,b])\n",
        "print([w.grad,b.grad])\n",
        "with torch.no_grad():\n",
        "    w -= lr * w.grad/batch_size # w = w - lr*w.grad \n",
        "    b -= lr * b.grad/batch_size # b = b - lr*b.grad\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "print([w,b])\n",
        "print([w.grad,b.grad])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining minibatch stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sgd(params, lr, batch_size):  \n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad/batch_size\n",
        "            param.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Linear regression model 학습을 위한 model architecture, loss function, optimization algorithm이 모두 준비가 되었습니다.\n",
        "- 3개의 example로 구성된 간단한 데이터를 이용해 학습을 진행하는 과정을 살펴 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# initial parameters\n",
            "tensor([1., 1., 1.], requires_grad=True) tensor([0.], requires_grad=True)\n",
            "\n",
            "\n",
            "## first prediction using initial parameters ##\n",
            "tensor([1., 1., 1.], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([1.0000, 1.3333, 1.6667], requires_grad=True) tensor([1.], requires_grad=True)\n",
            "\n",
            "\n",
            "## second prediction using the updated parameters ##\n",
            "tensor([2.0000, 2.3333, 2.6667], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([0.6667, 1.2222, 1.7778], requires_grad=True) tensor([0.6667], requires_grad=True)\n",
            "\n",
            "\n",
            "## third prediction using the updated parameters ##\n",
            "tensor([1.3333, 1.8889, 2.4444], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([0.5556, 1.2593, 1.9630], requires_grad=True) tensor([0.7778], requires_grad=True)\n",
            "\n",
            "\n",
            "## fourth prediction using the updated parameters ##\n",
            "tensor([1.3333, 2.0370, 2.7407], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([0.4444, 1.2469, 2.0494], requires_grad=True) tensor([0.7407], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "lr = 1\n",
        "batch_size = 3\n",
        "\n",
        "y = torch.tensor([1.,2,3]) # label (ground truth)\n",
        "X = torch.eye(3)\n",
        "w = torch.tensor([1.,1,1],requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "\n",
        "print('# initial parameters')\n",
        "print(w,b)\n",
        "\n",
        "# 1st iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## first prediction using initial parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)\n",
        "\n",
        "# 2nd iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## second prediction using the updated parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)\n",
        "\n",
        "\n",
        "# 3rd iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## third prediction using the updated parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)\n",
        "\n",
        "\n",
        "# 4th iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## fourth prediction using the updated parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Synthetic Data & Defining Data Loader\n",
        "- 실제로 linear 관계를 가지는 input feature와 label을 시뮬레이션을 통해 생성합니다.\n",
        "- 현실적인 상황에 좀 더 가깝도록 data에 노이즈를 추가하여 봅니다.\n",
        "- minibatch size를 기반으로 랜덤으로 데이터를 추출하여 가져올 수 있는 data loader를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = torch.normal(0,2,(1000,2))\n",
        "y = torch.matmul(X,torch.tensor([3.,4]))+5 # y = 3x1 + 4x2 + 5\n",
        "y += torch.normal(0,0.01,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(121)\n",
        "plt.scatter(X[:,0],y)\n",
        "plt.xlabel('first feature')\n",
        "plt.ylabel('target')\n",
        "plt.subplot(122)\n",
        "plt.scatter(X[:,1],y)\n",
        "plt.xlabel('second feature')\n",
        "plt.ylabel('target')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        batch_indices = torch.tensor(indices[i:min(i +\n",
        "                                                   batch_size, num_examples)])\n",
        "        yield features[batch_indices], labels[batch_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "batch_size=4\n",
        "for X_tr, y_tr in data_iter(batch_size,X,y):\n",
        "    print(X_tr.shape)\n",
        "    print(y_tr.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train a Model\n",
        "- 모델의 파라미터 w, b 를 모두 0으로 초기화하고, 하이퍼파라미터들(num_epochs, batch_size, learning rate)을 정해 줍니다.\n",
        "- 미리 정의한 network architecture, loss function을 가져오고 학습을 진행해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = torch.tensor([0.,0.],requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "num_epochs = 3\n",
        "batch_size = 16\n",
        "lr = 0.1\n",
        "net = linreg\n",
        "loss = squared_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "w = torch.tensor([1.,1.],requires_grad = True)\n",
        "b = torch.zeros(1.,requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.000071\n",
            "epoch 2, loss 0.000053\n",
            "epoch 3, loss 0.000052\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for X_tr, y_tr in data_iter(batch_size, X, y):\n",
        "        l = loss(net(X_tr, w, b), y_tr)  # Minibatch loss in `X_tr` and `y_tr`\n",
        "        # Compute gradient on `l` with respect to [`w`, `b`]\n",
        "        l.sum().backward()\n",
        "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
        "    with torch.no_grad():\n",
        "        train_l = loss(net(X, w, b), y)\n",
        "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 학습된 모델의 파라미터가 우리가 만든 데이터의 w1=3, w2=4, b=5와 맞는 지 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9999, grad_fn=<SelectBackward0>) tensor(4.0002, grad_fn=<SelectBackward0>) tensor([5.0000], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(w[0],w[1],b)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ch03_LinearRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
