{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuO1zyw15vKlw+tShkGua7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hufsaim/T10402201/blob/master/notebook/Lab02_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model\n",
        "- 이번 실습은 linear regression model을 직접 구현하여 mini-batch stochastic gradient descent 방법을 이용해서 model을 학습시키는 방법을 살펴봅니다."
      ],
      "metadata": {
        "id": "OKw_Aw5iv04S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u7jB9vWOv0F9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Linear Regression Model\n",
        "- Matrix X는 각 row에 각각의 example이 가지고 있는 input feature들을 포함합니다.\n",
        "- vector w는 각각의 feature에 해당하는 weight입니다.\n",
        "- b는 bias로 모든 feature가 0일 때의 출력값입니다.\n",
        "- w와 b는 우리가 학습을 시켜야 되는 파라미터들이기 때문에 requires_grad = True 를 포함시켜야 합니다.\n",
        "- pytorch에서는 requires_grad = True인 tensor들에 대해 자동으로 gradient를 계산할 수 있습니다."
      ],
      "metadata": {
        "id": "AVtkUz8Cv_ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear regression model y = Xw + b\n",
        "def linreg(X, w, b):\n",
        "    return torch.matmul(X,w) + b"
      ],
      "metadata": {
        "id": "5jDG3YWUv9j5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.eye(3)\n",
        "w = torch.normal(0,0.01,size=(3,1),requires_grad = True)\n",
        "#w = torch.tensor([1.,1,1],requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "\n",
        "print(\"X:\",X)\n",
        "print(\"w:\",w)\n",
        "print(\"b:\",b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezPNpsxnwEYT",
        "outputId": "81f85979-dc69-430d-e4f7-34714787ca27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "w: tensor([[-0.0077],\n",
            "        [-0.0122],\n",
            "        [-0.0162]], requires_grad=True)\n",
            "b: tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = linreg(X,w,b)\n",
        "print(\"y:\",y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpw7OqNIwGkv",
        "outputId": "40cc0808-6300-43b0-c39a-b5a1f6ce74c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: tensor([[-0.0077],\n",
            "        [-0.0122],\n",
            "        [-0.0162]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Squared Loss\n",
        "- tensor의 연산에서 tensor의 shape를 맞추는 것은 매우 중요합니다.\n",
        "- tensor의 shape가 맞지 않는 경우 임의로 broadcasting이 진행되어, 의도하지 않은 결과를 얻을 수 있습니다."
      ],
      "metadata": {
        "id": "BZtOgqbdwJde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = torch.normal(0,0.1,size=(3,1))\n",
        "y = torch.tensor([1.,2,3])\n",
        "\n",
        "print((y-y_hat)**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghQ4m8bXwIxQ",
        "outputId": "ae302d6a-6ab7-4889-d52f-3c81c8a4025b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1650, 4.3237, 9.4824],\n",
            "        [0.7369, 3.4537, 8.1706],\n",
            "        [0.9049, 3.8075, 8.7100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = torch.normal(0,0.1,size=(3,1))\n",
        "y = torch.tensor([1.,2,3],)\n",
        "\n",
        "print( (y_hat-y.reshape(y_hat.shape))**2 /2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC8lT-EQwNfj",
        "outputId": "80408872-e5b4-4040-d36a-0c25cf1008a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4614],\n",
            "        [1.9234],\n",
            "        [3.9592]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def squared_loss(y_hat, y):\n",
        "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
      ],
      "metadata": {
        "id": "sEnupOMpwPQr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squared_loss(y_hat,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qStQMzInwYOM",
        "outputId": "939fab05-c498-4ec1-e017-76315e46fb45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4614],\n",
              "        [1.9234],\n",
              "        [3.9592]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Gradients\n",
        "- 정의한 loss function을 이용하여 w, b에 대한 gradient를 계산합니다.\n"
      ],
      "metadata": {
        "id": "5rFjk2JhwafC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([1.,2,3]) # label (ground truth)\n",
        "X = torch.eye(3)\n",
        "w = torch.tensor([1.,1,1],requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "\n",
        "print('# initial parameters')\n",
        "print(w,b)\n",
        "print('# initial gradients: None')\n",
        "print(w.grad, b.grad)\n",
        "\n",
        "y_hat = linreg(X,w,b)\n",
        "print('# first prediction using initial parameters')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "print('# calculated loss')\n",
        "print(l.sum())\n",
        "\n",
        "print('# calculated gradients using the loss')\n",
        "print(w.grad, b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAh4tHwawZgb",
        "outputId": "fbcd402b-a433-4c59-820a-29ed047dd9fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# initial parameters\n",
            "tensor([1., 1., 1.], requires_grad=True) tensor([0.], requires_grad=True)\n",
            "# initial gradients: None\n",
            "None None\n",
            "# first prediction using initial parameters\n",
            "tensor([1., 1., 1.], grad_fn=<AddBackward0>)\n",
            "# calculated loss\n",
            "tensor(2.5000, grad_fn=<SumBackward0>)\n",
            "# calculated gradients using the loss\n",
            "tensor([ 0., -1., -2.]) tensor([-3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 계산된 gradients를 이용해 파라미터를 새로 업데이트 합니다.\n",
        "- 파라미터를 업데이트한 후에는 각각의 파라미터의 gradient를 다시 초기화 해주어야 합니다."
      ],
      "metadata": {
        "id": "fQ0E-GjOwpjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1\n",
        "batch_size = 3\n",
        "print([w,b])\n",
        "print([w.grad,b.grad])\n",
        "with torch.no_grad():\n",
        "    w -= lr * w.grad/batch_size # w = w - lr*w.grad \n",
        "    b -= lr * b.grad/batch_size # b = b - lr*b.grad\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "print([w,b])\n",
        "print([w.grad,b.grad])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H6p1J_iwirE",
        "outputId": "30a5d02a-150f-4b66-bb22-f5aeb55f492f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([1., 1., 1.], requires_grad=True), tensor([0.], requires_grad=True)]\n",
            "[tensor([ 0., -1., -2.]), tensor([-3.])]\n",
            "[tensor([1.0000, 1.3333, 1.6667], requires_grad=True), tensor([1.], requires_grad=True)]\n",
            "[tensor([0., 0., 0.]), tensor([0.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining minibatch stochastic gradient descent\n"
      ],
      "metadata": {
        "id": "CnD07IGdwsiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(params, lr, batch_size):  \n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad/batch_size\n",
        "            param.grad.zero_()"
      ],
      "metadata": {
        "id": "LfQ4xUZ6wrwX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 Linear regression model 학습을 위한 model architecture, loss function, optimization algorithm이 모두 준비가 되었습니다.\n",
        "- 3개의 example로 구성된 간단한 데이터를 이용해 학습을 진행하는 과정을 살펴 봅니다."
      ],
      "metadata": {
        "id": "VO2P4cMHwvsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1\n",
        "batch_size = 3\n",
        "\n",
        "y = torch.tensor([1.,2,3]) # label (ground truth)\n",
        "X = torch.eye(3)\n",
        "w = torch.tensor([1.,1,1],requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "\n",
        "print('# initial parameters')\n",
        "print(w,b)\n",
        "\n",
        "# 1st iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## first prediction using initial parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)\n",
        "\n",
        "# 2nd iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## second prediction using the updated parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)\n",
        "\n",
        "\n",
        "# 3rd iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## third prediction using the updated parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)\n",
        "\n",
        "\n",
        "# 4th iteration\n",
        "y_hat = linreg(X,w,b)\n",
        "print('\\n')\n",
        "print('## fourth prediction using the updated parameters ##')\n",
        "print(y_hat)\n",
        "\n",
        "l = squared_loss(y_hat, y)\n",
        "l.sum().backward()\n",
        "sgd([w,b],lr,batch_size)\n",
        "print('# updated parameters')\n",
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f937u4I3wuXB",
        "outputId": "797727e3-fd3d-4d86-9943-6686489f1883"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# initial parameters\n",
            "tensor([1., 1., 1.], requires_grad=True) tensor([0.], requires_grad=True)\n",
            "\n",
            "\n",
            "## first prediction using initial parameters ##\n",
            "tensor([1., 1., 1.], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([1.0000, 1.3333, 1.6667], requires_grad=True) tensor([1.], requires_grad=True)\n",
            "\n",
            "\n",
            "## second prediction using the updated parameters ##\n",
            "tensor([2.0000, 2.3333, 2.6667], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([0.6667, 1.2222, 1.7778], requires_grad=True) tensor([0.6667], requires_grad=True)\n",
            "\n",
            "\n",
            "## third prediction using the updated parameters ##\n",
            "tensor([1.3333, 1.8889, 2.4444], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([0.5556, 1.2593, 1.9630], requires_grad=True) tensor([0.7778], requires_grad=True)\n",
            "\n",
            "\n",
            "## fourth prediction using the updated parameters ##\n",
            "tensor([1.3333, 2.0370, 2.7407], grad_fn=<AddBackward0>)\n",
            "# updated parameters\n",
            "tensor([0.4444, 1.2469, 2.0494], requires_grad=True) tensor([0.7407], requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}