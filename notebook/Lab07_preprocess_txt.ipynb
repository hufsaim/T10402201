{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzi0ZmZaQn4VyoMRc8zJy3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hufsaim/T10402201/blob/master/notebook/Lab07_preprocess_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "TBt6nkLlTsoa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5fMygZImPgJe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "import collections\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        counter = count_corpus(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        self.unk, uniq_tokens = 0, [''] + reserved_tokens\n",
        "        uniq_tokens += [\n",
        "            token for token, freq in self.token_freqs\n",
        "            if freq >= min_freq and token not in uniq_tokens]\n",
        "        self.idx_to_token, self.token_to_idx = [], dict()\n",
        "        for token in uniq_tokens:\n",
        "            self.idx_to_token.append(token)\n",
        "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "def count_corpus(tokens):\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)\n",
        "\n",
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, 'char')\n",
        "    vocab = Vocab(tokens)\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    random.shuffle(initial_indices)\n",
        "    def data(pos):\n",
        "        return corpus[pos:pos + num_steps]\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield torch.tensor(X), torch.tensor(Y)\n",
        ""
      ],
      "metadata": {
        "id": "iOr7614zTB70"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lines, token='word'):\n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)"
      ],
      "metadata": {
        "id": "tl09wXFRTFkE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임의의 txt 파일을 불러 옵니다.\n"
      ],
      "metadata": {
        "id": "MZjpRf5XTx2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path0 = 'timemachine.txt' # replace your own path\n",
        "with open(path0, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "lines\n",
        "[re.sub('[^A-Za-z]+',' ',line).strip().lower() for line in lines]\n",
        "\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTZXbH4TTHch",
        "outputId": "83ca4853-4a30-40d8-c685-b030e3a4e71d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# text lines: 3617\n",
            "Project Gutenberg's The Time Machine, by H. G. (Herbert George) Wells\n",
            "\n",
            "Author: H. G. (Herbert George) Wells\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "line별로 token으로 분리합니다.\n"
      ],
      "metadata": {
        "id": "L0-u4ePTT2H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenize(lines)\n",
        "for i in range(11):\n",
        "    print(tokens[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuiU5tvbTRjs",
        "outputId": "e6971757-d49f-40ba-ae7b-9f1384ef14e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Project', \"Gutenberg's\", 'The', 'Time', 'Machine,', 'by', 'H.', 'G.', '(Herbert', 'George)', 'Wells']\n",
            "[]\n",
            "['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with']\n",
            "['almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or']\n",
            "['re-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included']\n",
            "['with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.net']\n",
            "[]\n",
            "[]\n",
            "['Title:', 'The', 'Time', 'Machine']\n",
            "[]\n",
            "['Author:', 'H.', 'G.', '(Herbert', 'George)', 'Wells']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab을 생성합니다.\n"
      ],
      "metadata": {
        "id": "XaG6S1xTT4Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iK8sUjOTUq3",
        "outputId": "3970482f-274a-47ee-893c-0aafc9a076b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('', 0), ('the', 1), ('of', 2), ('and', 3), ('I', 4), ('a', 5), ('to', 6), ('in', 7), ('was', 8), ('my', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0, 10]:\n",
        "    print('words:', tokens[i])\n",
        "    print('indices:', vocab[tokens[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snPNidwbTWeO",
        "outputId": "8d343cdc-d991-40ef-eff1-13290264ec92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words: ['Project', \"Gutenberg's\", 'The', 'Time', 'Machine,', 'by', 'H.', 'G.', '(Herbert', 'George)', 'Wells']\n",
            "indices: [50, 1635, 17, 33, 298, 29, 867, 868, 1152, 1153, 869]\n",
            "words: ['Author:', 'H.', 'G.', '(Herbert', 'George)', 'Wells']\n",
            "indices: [2771, 867, 868, 1152, 1153, 869]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = -1\n",
        "corpus = [vocab[token] for line in tokens for token in line]\n",
        "if max_tokens > 0:\n",
        "  corpus = corpus[:max_tokens]\n",
        "\n",
        "len(corpus), len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRayaijDTYH5",
        "outputId": "41423614-0599-4d30-80e2-87d0a95e8e89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35319, 7636)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝모델의 학습에 활용하기 위한 data loader가 제대로 작동하는지 확인합니다.\n"
      ],
      "metadata": {
        "id": "0E3U0ytaT7Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in seq_data_iter_random(corpus[:14], batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtbdThoWTZqo",
        "outputId": "cf744087-0e6e-4546-ebaa-c78cf5f85543"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  tensor([[  33,  298,   29,  867,  868],\n",
            "        [1152, 1153,  869,  219,  703]]) \n",
            "Y: tensor([[ 298,   29,  867,  868, 1152],\n",
            "        [1153,  869,  219,  703,   27]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab을 통하여, 빈도수 기준 상위 10개의 token을 확인합니다.\n"
      ],
      "metadata": {
        "id": "pN5VRomeT8va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for v in range(0,10):\n",
        "  print(vocab.idx_to_token[v])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se4adGeTTbh8",
        "outputId": "52b1c889-2624-45f5-b8b2-41bd35dd1083"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "the\n",
            "of\n",
            "and\n",
            "I\n",
            "a\n",
            "to\n",
            "in\n",
            "was\n",
            "my\n"
          ]
        }
      ]
    }
  ]
}